{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-05T09:04:24.652748Z","iopub.status.busy":"2024-08-05T09:04:24.652182Z","iopub.status.idle":"2024-08-05T09:04:39.920522Z","shell.execute_reply":"2024-08-05T09:04:39.919412Z","shell.execute_reply.started":"2024-08-05T09:04:24.652721Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting trl==0.9.6\n","  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n","Collecting peft==0.12.0\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Collecting gdown\n","  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6) (2.1.2)\n","Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6) (4.42.3)\n","Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6) (1.26.4)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6) (0.32.1)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.9.6) (2.20.0)\n","Collecting tyro>=0.5.11 (from trl==0.9.6)\n","  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (6.0.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (4.66.4)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.0) (0.23.4)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.12.0) (3.1.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.9.6) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.9.6) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.9.6) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.9.6) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.9.6) (0.19.1)\n","Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.9.6)\n","  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.9.6) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.6)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (2.2.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.9.6) (3.9.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.9.6) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.9.6) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.9.6) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.9.6) (2023.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.9.6) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.16.0)\n","Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\n","Downloading tyro-0.8.5-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, docstring-parser, tyro, gdown, trl, peft\n","  Attempting uninstall: docstring-parser\n","    Found existing installation: docstring-parser 0.15\n","    Uninstalling docstring-parser-0.15:\n","      Successfully uninstalled docstring-parser-0.15\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed docstring-parser-0.16 gdown-5.2.0 peft-0.12.0 shtab-1.7.1 trl-0.9.6 tyro-0.8.5\n"]}],"source":["!pip install trl==0.9.6 peft==0.12.0 gdown"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:04:59.803704Z","iopub.status.busy":"2024-08-05T09:04:59.803001Z","iopub.status.idle":"2024-08-05T09:04:59.832742Z","shell.execute_reply":"2024-08-05T09:04:59.831893Z","shell.execute_reply.started":"2024-08-05T09:04:59.803669Z"},"trusted":true},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:08:35.561757Z","iopub.status.busy":"2024-08-05T09:08:35.561026Z","iopub.status.idle":"2024-08-05T09:08:35.586603Z","shell.execute_reply":"2024-08-05T09:08:35.585831Z","shell.execute_reply.started":"2024-08-05T09:08:35.561721Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting configs.py\n"]}],"source":["%%writefile configs.py\n","\n","from dataclasses import dataclass, field\n","\n","\n","@dataclass\n","class SFTArgs:\n","    save_folder: str = field(default='sft', metadata={'help': 'Folder to save SFT model.'})\n","\n","\n","@dataclass\n","class RewardConfigArgs:\n","    output_dir: str = field(default='reward', metadata={'help': 'The output directory where the model checkpoints will be written.'})\n","    per_device_train_batch_size: int = field(default=32, metadata={'help': 'The batch size for training.'})\n","    gradient_accumulation_steps: int = field(default=1, metadata={'help': 'Number of updates steps to accumulate the gradients for.'})\n","    learning_rate: float = field(default=1.41e-5, metadata={'help': 'The initial learning rate for AdamW optimizer.'})\n","    max_steps: int = field(default=4000, metadata={'help': 'The total number of training steps to perform.'})\n","    logging_steps: int = field(default=100, metadata={'help': 'Number of update steps between two logs.'})\n","    gradient_checkpointing: bool = field(default=True, metadata={'help': 'If True, use gradient checkpointing.'})\n","    fp16: bool = field(default=True, metadata={'help': 'Whether to use 16-bit (mixed) precision training.'})\n","    bf16: bool = field(default=False, metadata={'help': 'Whether to use bf16 16-bit (mixed) precision training.'})\n","    max_length: int = field(default=512, metadata={'help': 'The maximum length of the sequences in the batch.'})\n","\n","\n","@dataclass\n","class PolicyTrainArgs:\n","    per_device_train_batch_size: int = field(default=64, metadata={'help': 'The batch size for training.'})\n","    per_device_eval_batch_size: int = field(default=32, metadata={'help': 'The batch size for evaluation.'})\n","    gradient_accumulation_steps: int = field(default=1, metadata={'help': 'Number of updates steps to accumulate the gradients for.'})\n","    learning_rate: float = field(default=1.41e-5, metadata={'help': 'The initial learning rate for AdamW optimizer.'})\n","    max_steps: int = field(default=100, metadata={'help': '(T) Number of train iterations for each policy.'})\n","    logging_steps: int = field(default=10, metadata={'help': 'Number of update steps between two logs.'})\n","    gradient_checkpointing: bool = field(default=True, metadata={'help': 'If True, use gradient checkpointing.'})\n","    fp16: bool = field(default=False, metadata={'help': 'Whether to use fp16 16-bit (mixed) precision training.'})\n","    bf16: bool = field(default=False, metadata={'help': 'Whether to use bf16 16-bit (mixed) precision training.'})\n","    warmup_steps: int = field(default=0, metadata={'help': 'Linear warmup over warmup_steps.'})\n","\n","\n","@dataclass\n","class LoraArgs:\n","    rank: int = field(default=32, metadata={'help': 'Lora attention dimension.'})\n","    lora_alpha: int = field(default=32, metadata={'help': 'The alpha parameter for Lora scaling.'})\n","    lora_dropout: float = field(default=0.0, metadata={'help': 'The dropout probability for Lora layers.'})\n","\n","\n","@dataclass\n","class CheckpointsArgs:\n","    group_name: str = field(default='warp', metadata={'help': 'Group name for WAND runs. Should be unique every time, otherwise logs to the existing group.'})\n","    sft_checkpoint: str = field(default='sft', metadata={'help': 'Path to sft model/tokenizer checkpoint.'})\n","    reward_checkpoint: str = field(default='reward', metadata={'help': 'Path to reward model/tokenizer checkpoint.'})\n","    save_folder: str = field(default='warp', metadata={'help': 'Folder to save WARP output.'})\n","\n","\n","@dataclass\n","class DatasetArgs:\n","    min_text_length: int = field(default=200, metadata={'help': 'Minimum length of a review from imdb dataset.'})\n","    min_tokens: int = field(default=5, metadata={'help': 'Minimum number of tokens after prompt truncation.'})\n","    max_tokens: int = field(default=20, metadata={'help': 'Maximum number of tokens after prompt truncation.'})\n","    eval_size: int = field(default=100, metadata={'help': 'Evaluation subset size.'})\n","\n","\n","@dataclass\n","class GenerationArgs:\n","    max_new_tokens: int = field(default=64, metadata={'help': 'The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.'})\n","    do_sample: bool = field(default=True, metadata={'help': 'Whether or not to use sampling ; use greedy decoding otherwise.'})\n","    top_k: int | None = field(default=None, metadata={'help': 'The number of highest probability vocabulary tokens to keep for top-k-filtering.'})\n","    top_p: float | None = field(default=1.0, metadata={'help': 'Only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.'})\n","    temperature: float | None = field(default=1.0, metadata={'help': 'The value used to modulate the next token probabilities.'})\n","    num_return_sequences: int = field(default=1, metadata={'help': 'The number of independently computed returned sequences for each element in the batch'})\n","\n","\n","@dataclass\n","class WARPArgs:\n","    num_iterations: int = field(default=2, metadata={'help': '(I) Number of WARP iterations.'})\n","    num_policies: int = field(default=2, metadata={'help': '(M) Number of policies to train in parallel.'})\n","    kl_coef: float = field(default=0.1, metadata={'help': '(beta) Weight of KL for KL-regularized reward.'})\n","    ema_rate: float = field(default=0.01, metadata={'help': '(mu) EMA rate for reference policies.'})\n","    slerp_rate: float = field(default=0.5, metadata={'help': '(lambda) Weight to SLERP trained policies. Ignored when M > 2 (lambda = 1/M).'})\n","    liti_rate: float = field(default=0.5, metadata={'help': '(eta) LITI rate for initial policy.'})\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:05:03.478668Z","iopub.status.busy":"2024-08-05T09:05:03.478283Z","iopub.status.idle":"2024-08-05T09:05:03.497989Z","shell.execute_reply":"2024-08-05T09:05:03.497063Z","shell.execute_reply.started":"2024-08-05T09:05:03.478637Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing utils.py\n"]}],"source":["%%writefile utils.py\n","\n","import os\n","import re\n","import torch\n","import wandb\n","from wandb.sdk.wandb_run import Run\n","\n","\n","def compute_angle(v1: torch.Tensor, v2: torch.Tensor) -> torch.Tensor:\n","    cos = torch.sum(v1 * v2) / torch.norm(v1) / torch.norm(v2)\n","    return torch.acos(cos)\n","\n","\n","def get_latest_checkpoint(dir: str | os.PathLike) -> str:\n","    pattern = r'checkpoint-\\d+'\n","    checkpoints = [file for file in os.scandir(dir) if file.is_dir() and re.fullmatch(pattern, file.name)]\n","    if not checkpoints:\n","        return dir\n","\n","    latest_checkpoint = max(int(checkpoint.name.split('-')[1]) for checkpoint in checkpoints)\n","    return os.path.join(dir, f'checkpoint-{latest_checkpoint}')\n","\n","\n","def print_wandb_run(run: Run):\n","    wandb_path = '/'.join(run.dir.split('/')[:-2])\n","    group_url = f'{run.get_project_url()}/groups/{run.group}'\n","\n","    info = [\n","        f'Currently logged in as: {run.entity}',\n","        f'Tracking run with wandb version {wandb.__version__}',\n","        f'Run data is saved locally in {wandb_path}',\n","        f'View project at {run.get_project_url()}',\n","        f'View runs at {group_url}'\n","    ]\n","\n","    print(*info, sep='\\n')\n","\n","\n","def is_lora_layer(name: str) -> bool:\n","    return name.find('lora_A') != -1 or name.find('lora_B') != -1\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:05:05.312683Z","iopub.status.busy":"2024-08-05T09:05:05.312318Z","iopub.status.idle":"2024-08-05T09:05:05.331734Z","shell.execute_reply":"2024-08-05T09:05:05.330862Z","shell.execute_reply.started":"2024-08-05T09:05:05.312654Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing dataset.py\n"]}],"source":["%%writefile dataset.py\n","\n","import typing as tp\n","import torch\n","import trl\n","import datasets\n","from torch.utils.data import Dataset\n","from transformers import PreTrainedTokenizer\n","\n","\n","class RewardDatasetItem(tp.TypedDict):\n","    input_ids_chosen: list[int]\n","    attention_mask_chosen: list[int]\n","    input_ids_rejected: list[int]\n","    attention_mask_rejected: list[int]\n","\n","\n","class DatasetItem(tp.TypedDict):\n","    text: str\n","    query: str\n","    input_ids: torch.Tensor\n","\n","\n","class IMDBRewardDataset(Dataset):\n","    def __init__(self, tokenizer: PreTrainedTokenizer, accepted_label: int, split: str = 'train'):\n","        super().__init__()\n","\n","        imdb_dataset = datasets.load_dataset('stanfordnlp/imdb', split=split)\n","        self.tokenizer = tokenizer\n","        self.chosen_texts = [row['text'] for row in imdb_dataset if row['label'] == accepted_label]\n","        self.rejected_texts = [row['text'] for row in imdb_dataset if row['label'] != accepted_label]\n","\n","        self.n_chosen = len(self.chosen_texts)\n","        self.n_rejected = len(self.rejected_texts)\n","\n","    def __len__(self):\n","        return self.n_chosen * self.n_rejected\n","\n","    def __getitem__(self, index: int) -> RewardDatasetItem:\n","        chosen = self.tokenizer(self.chosen_texts[index // self.n_rejected], truncation=True)\n","        rejected = self.tokenizer(self.rejected_texts[index % self.n_rejected], truncation=True)\n","\n","        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n","                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])\n","\n","\n","def select_query_and_tokenize(sample: DatasetItem, tokenizer: PreTrainedTokenizer, length_sampler: trl.core.LengthSampler):\n","    query_ids = tokenizer.encode(sample['text'])[:length_sampler()]\n","    sample[\"query\"] = tokenizer.decode(query_ids)\n","    sample[\"input_ids\"] = query_ids\n","    return sample\n","\n","\n","def build_imdb_dataset(tokenizer: PreTrainedTokenizer, min_text_length: int = 200, min_tokens: int = 5, max_tokens: int = 15) -> Dataset[DatasetItem]:\n","    imdb_dataset = datasets.load_dataset('stanfordnlp/imdb')\n","    length_sampler = trl.core.LengthSampler(min_tokens, max_tokens)\n","\n","    imdb_dataset = imdb_dataset.filter(lambda row: len(row['text']) > min_text_length, batched=False)\n","    # Need to have label column to make compute_metrics work\n","    # imdb_dataset = imdb_dataset.remove_columns(['label'])\n","    imdb_dataset = imdb_dataset.map(lambda sample: select_query_and_tokenize(sample, tokenizer, length_sampler), batched=False)\n","    imdb_dataset.set_format(type='torch')\n","\n","    return imdb_dataset\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:05:06.351975Z","iopub.status.busy":"2024-08-05T09:05:06.351610Z","iopub.status.idle":"2024-08-05T09:05:06.374381Z","shell.execute_reply":"2024-08-05T09:05:06.373485Z","shell.execute_reply.started":"2024-08-05T09:05:06.351945Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing policy_trainer.py\n"]}],"source":["%%writefile policy_trainer.py\n","\n","import torch\n","import torch.optim.swa_utils as swa\n","import transformers\n","from collections import defaultdict\n","from torch.utils.data import Dataset\n","from configs import WARPArgs\n","\n","\n","class EMAStepCallback(transformers.TrainerCallback):\n","    def __init__(self, model: transformers.PreTrainedModel, ema_model: swa.AveragedModel):\n","        self._model = model\n","        self._ema_model = ema_model\n","\n","    def on_step_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n","        self._ema_model.update_parameters(self._model)\n","\n","\n","class PolicyTrainer(transformers.Trainer):\n","    EPS = 1e-7\n","    INVALID_LOGPROB = 0.0\n","    INVALID_REWARD = -1.0\n","\n","    def __init__(\n","        self,\n","        policy: transformers.PreTrainedModel,\n","        policy_tokenizer: transformers.PreTrainedTokenizer,\n","        reward_model: transformers.PreTrainedModel,\n","        reward_tokenizer: transformers.PreTrainedTokenizer,\n","        generation_config: transformers.GenerationConfig,\n","        warp_args: WARPArgs,\n","        ref_policy: transformers.PreTrainedModel | None = None,\n","        training_args: transformers.TrainingArguments | None = None,\n","        train_dataset: Dataset | None = None,\n","        eval_dataset: Dataset | None = None,\n","        callbacks: list[transformers.TrainerCallback] | None = None,\n","    ):\n","        super().__init__(\n","            model=policy,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset,\n","            tokenizer=policy_tokenizer,\n","            callbacks=callbacks,\n","            compute_metrics=self._compute_metrics,\n","        )\n","\n","        self.reward_model = reward_model\n","        self.reward_tokenizer = reward_tokenizer\n","        self.generation_config = generation_config\n","        self.warp_args = warp_args\n","        self._metrics = defaultdict(list)\n","\n","        if ref_policy is None:\n","            self.ref_policy = swa.AveragedModel(self.model, multi_avg_fn=swa.get_ema_multi_avg_fn(1 - warp_args.ema_rate))\n","            self.add_callback(EMAStepCallback(self.model, self.ref_policy))\n","        else:\n","            self.ref_policy = ref_policy\n","\n","    def compute_loss(self, model: transformers.PreTrainedModel, inputs: transformers.BatchEncoding, return_outputs: bool = False):\n","        prompt_length = inputs['input_ids'].shape[1]\n","        gen_tokens, gen_logprobs = self._generate(model, inputs, prompt_length)\n","\n","        policy_logprobs = self._forward(model, gen_tokens, prompt_length)\n","        with torch.no_grad():\n","            ref_logprobs = self._forward(self.ref_policy, gen_tokens, prompt_length)\n","\n","        kl = torch.sum(gen_logprobs - ref_logprobs, dim=1)\n","        reward = self._get_reward(gen_tokens)\n","        rlhf_reward = reward - self.warp_args.kl_coef * kl\n","\n","        batch_metrics = {\n","            'batch_kl': kl.mean().item(),\n","            'batch_reward': reward.mean().item(),\n","            'batch_rlhf_reward': rlhf_reward.mean().item()\n","        }\n","        self.log(batch_metrics)\n","\n","        if self.generation_config.num_return_sequences > 1:\n","            loss = self._rloo_loss(rlhf_reward, policy_logprobs.sum(dim=-1))\n","        else:\n","            loss = -torch.mean(rlhf_reward * policy_logprobs.sum(dim=-1))\n","\n","        return (loss, policy_logprobs) if return_outputs else loss\n","\n","    def _rloo_loss(self, rlhf_reward: torch.Tensor, policy_logprobs: torch.Tensor) -> torch.Tensor:\n","        k = self.generation_config.num_return_sequences\n","        rlhf_reward = rlhf_reward.reshape((-1, k))\n","        policy_logprobs = policy_logprobs.reshape((-1, k))\n","\n","        baselines = (rlhf_reward.sum(dim=-1, keepdim=True) - rlhf_reward) / (k - 1)\n","        return -torch.mean((rlhf_reward - baselines) * policy_logprobs)\n","\n","    def _compute_metrics(self, pred: transformers.EvalPrediction, compute_result: bool = False) -> dict[str, float]:\n","        prompt_length = pred.inputs['input_ids'].shape[1]\n","        gen_tokens, gen_logprobs = self._generate(self.model, pred.inputs, prompt_length)\n","\n","        with torch.no_grad():\n","            ref_logprobs = self._forward(self.ref_policy, gen_tokens, prompt_length)\n","\n","        kl = torch.sum(gen_logprobs - ref_logprobs, dim=1)\n","        reward = self._get_reward(gen_tokens)\n","\n","        self._metrics['KL'].extend(kl.cpu().tolist())\n","        self._metrics['Reward'].extend(reward.cpu().tolist())\n","        batch_metrics = {'KL': kl.mean().item(), 'Reward': reward.mean().item()}\n","\n","        if not compute_result:\n","            return batch_metrics\n","\n","        final_metrics = {name: sum(values) / len(values) for name, values in self._metrics.items()}\n","        self._metrics = defaultdict(list)\n","        return  final_metrics\n","\n","    def _generate(self, model: transformers.PreTrainedModel, inputs: transformers.BatchEncoding, prompt_length: int) -> tuple[torch.Tensor, torch.Tensor]:\n","        generate_out = model.generate(\n","            inputs=inputs['input_ids'],\n","            attention_mask=inputs['attention_mask'],\n","            generation_config=self.generation_config,\n","            pad_token_id=self.tokenizer.pad_token_id,\n","            use_cache=False,\n","            return_dict_in_generate=True,\n","            output_scores=True,\n","        )\n","\n","        generated_tokens = generate_out.sequences\n","        generated_logprobs = self._process_generate_logits(\n","            torch.stack(generate_out.scores, dim=1),\n","            generated_tokens,\n","            prompt_length,\n","        )\n","\n","        return generated_tokens, generated_logprobs\n","\n","    def _process_generate_logits(self, logits: torch.Tensor, gen_tokens: torch.Tensor, prompt_length: int) -> torch.Tensor:\n","        generated_pad_mask = gen_tokens[:, prompt_length:] == self.tokenizer.pad_token_id\n","        all_logprobs = torch.nn.functional.log_softmax(logits, dim=-1)\n","        logprobs = torch.gather(all_logprobs, 2, gen_tokens[:, prompt_length:].unsqueeze(-1)).squeeze(-1)\n","        logprobs[generated_pad_mask] = self.INVALID_LOGPROB\n","        return logprobs\n","\n","    def _forward(self, model: transformers.PreTrainedModel, gen_tokens: torch.Tensor, prompt_length: int) -> torch.Tensor:\n","        attention_mask = gen_tokens != self.tokenizer.pad_token_id\n","        position_ids = attention_mask.cumsum(-1) - 1\n","        position_ids.masked_fill_(attention_mask == 0, 1)\n","        model_out = model(input_ids=gen_tokens, attention_mask=attention_mask, position_ids=position_ids)\n","        logprobs = self._process_forward_logits(model_out.logits, gen_tokens, prompt_length)\n","        return logprobs\n","\n","    def _process_forward_logits(self, policy_out: torch.Tensor, gen_tokens: torch.Tensor, prompt_length: int) -> torch.Tensor:\n","        policy_logits = policy_out[:, prompt_length - 1: -1]\n","        generated_pad_mask = gen_tokens[:, prompt_length:] == self.tokenizer.pad_token_id\n","\n","        policy_logits /= (self.generation_config.temperature + self.EPS)\n","        all_logprobs = torch.nn.functional.log_softmax(policy_logits, dim=-1)\n","        logprobs = torch.gather(all_logprobs, 2, gen_tokens[:, prompt_length:].unsqueeze(-1)).squeeze(-1)\n","        logprobs[generated_pad_mask] = self.INVALID_LOGPROB\n","        return logprobs\n","\n","    @torch.no_grad()\n","    def _get_reward(self, generated_tokens: torch.Tensor) -> torch.Tensor:\n","        finished_sequence_mask = generated_tokens[:, -1] == self.tokenizer.pad_token_id\n","        text = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        reward_tokens = self.reward_tokenizer(text=text, truncation=True, padding=True, return_tensors='pt').to(self.reward_model.device)\n","\n","        logits = self.reward_model(**reward_tokens).logits\n","        rewards = torch.nn.functional.softmax(logits, dim=-1)[:, 0]\n","        rewards[~finished_sequence_mask] = self.INVALID_REWARD\n","\n","        return rewards\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:05:48.850140Z","iopub.status.busy":"2024-08-05T09:05:48.849811Z","iopub.status.idle":"2024-08-05T09:05:48.875052Z","shell.execute_reply":"2024-08-05T09:05:48.874193Z","shell.execute_reply.started":"2024-08-05T09:05:48.850114Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing warp_trainer.py\n"]}],"source":["%%writefile warp_trainer.py\n","\n","import os\n","import gc\n","import wandb\n","import torch\n","import torch.multiprocessing as tmp\n","import transformers\n","import peft\n","import configs, utils, policy_trainer\n","from dataclasses import asdict\n","from tqdm.auto import tqdm\n","from torch.utils.data import Dataset\n","\n","class AsyncProgressCallback(transformers.TrainerCallback):\n","    def __init__(self, queue: tmp.Queue):\n","        self._queue = queue\n","\n","    def on_step_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState, control: transformers.TrainerControl, **kwargs):\n","        self._queue.put(None)\n","\n","\n","class WARPTrainer:\n","    def __init__(\n","        self,\n","        warp_args: configs.WARPArgs,\n","        generation_args: configs.GenerationArgs,\n","        training_args: configs.PolicyTrainArgs,\n","        lora_args: configs.LoraArgs,\n","        checkpoints_args: configs.CheckpointsArgs,\n","        train_dataset: Dataset,\n","        eval_dataset: Dataset | None = None\n","    ):\n","        self.checkpoints_args = checkpoints_args\n","        self.generation_config = transformers.GenerationConfig.from_dict(asdict(generation_args))\n","        self.warp_args = warp_args\n","        self.train_args = training_args\n","        self.lora_config = peft.LoraConfig(\n","            task_type=peft.TaskType.CAUSAL_LM,\n","            r=lora_args.rank,\n","            lora_alpha=lora_args.lora_alpha,\n","            lora_dropout=lora_args.lora_dropout,\n","        )\n","\n","        self.init_policy_checkpoint = checkpoints_args.sft_checkpoint\n","        self.train_dataset = train_dataset\n","        self.eval_dataset = eval_dataset\n","\n","        manager = tmp.Manager()\n","        self._progress_queue = manager.Queue()\n","        self._device_queue = manager.Queue()\n","        self._print_event = manager.Event()\n","        self._eval_run_id = None\n","\n","        device_count = torch.cuda.device_count()\n","        for device_idx in range(device_count):\n","            self._device_queue.put(device_idx)\n","\n","    def train(self):\n","        steps_per_iteration = self.warp_args.num_policies * self.train_args.max_steps\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(\n","            utils.get_latest_checkpoint(self.init_policy_checkpoint),\n","            padding_side='left'\n","        )\n","\n","        for iter_idx in range(self.warp_args.num_iterations):\n","            with tmp.Pool(self._device_queue.qsize()) as pool:\n","                for run_idx in range(self.warp_args.num_policies):\n","                    pool.apply_async(self._train_policy, (iter_idx, run_idx))\n","\n","                self._print_event.wait()\n","                with tqdm(total=steps_per_iteration, desc=f'Iteration {iter_idx + 1}/{self.warp_args.num_iterations}') as progress_bar:\n","                    for _ in range(steps_per_iteration):\n","                        self._progress_queue.get()\n","                        progress_bar.update()\n","\n","                pool.close()\n","                pool.join()\n","\n","            slerp_model = self._slerp(iter_idx)\n","            slerp_path = self._slerp_path(iter_idx)\n","            slerp_model.save_pretrained(slerp_path)\n","            tokenizer.save_pretrained(slerp_path)\n","\n","            init_model = self._litti(slerp_model)\n","            self.init_policy_checkpoint = self._init_path(iter_idx + 1)\n","            init_model.save_pretrained(self.init_policy_checkpoint)\n","            tokenizer.save_pretrained(self.init_policy_checkpoint)\n","\n","            del slerp_model, init_model\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            self._evaluate_slerp_model(iter_idx)\n","\n","    def _train_policy(self, iter_idx: int, run_idx: int):\n","        device_idx = self._pop_device()\n","        device = torch.device('cuda')\n","        train_args = self._get_train_args(iter_idx, run_idx)\n","\n","        policy, policy_tokenizer = self._load_policy(self.init_policy_checkpoint, device_map=device)\n","        reward_model, reward_tokenizer = self._load_reward(device_map=device)\n","        callbacks = [AsyncProgressCallback(self._progress_queue)]\n","\n","        with wandb.init(group=self.checkpoints_args.group_name, job_type=train_args.run_name, name=train_args.run_name, project='tk-alignment') as run:\n","            if iter_idx == 0 and run_idx == 0:\n","                utils.print_wandb_run(run)\n","                self._print_event.set()\n","\n","            trainer = policy_trainer.PolicyTrainer(\n","                policy,\n","                policy_tokenizer,\n","                reward_model,\n","                reward_tokenizer,\n","                self.generation_config,\n","                self.warp_args,\n","                training_args=train_args,\n","                train_dataset=self.train_dataset,\n","                eval_dataset=self.eval_dataset,\n","                callbacks=callbacks\n","            )\n","\n","            trainer.remove_callback(transformers.trainer_callback.PrinterCallback)\n","            trainer.remove_callback(transformers.trainer_callback.ProgressCallback)\n","            trainer.train()\n","\n","        self._device_queue.put(device_idx)\n","\n","    def _litti(self, slerp_model: transformers.PreTrainedModel) -> transformers.PreTrainedModel:\n","        init_model, _ = self._load_policy(self.init_policy_checkpoint)\n","        for (name, init_param), slerp_param in zip(init_model.named_parameters(), slerp_model.parameters()):\n","            if utils.is_lora_layer(name):\n","                init_param = init_param.detach()\n","                init_param += self.warp_args.liti_rate * (slerp_param - init_param)\n","        return init_model\n","\n","    def _slerp(self, iter_idx: int) -> transformers.PreTrainedModel:\n","        if self.warp_args.num_policies == 1:\n","            return self._load_policy(self._policy_path(iter_idx, 0))\n","\n","        if self.warp_args.num_policies == 2:\n","            return self._slerp_first_two_policies(iter_idx)\n","\n","        # merge M > 2 policies\n","        init_model, _ = self._load_policy(self.init_policy_checkpoint)\n","        slerp_model = self._slerp_first_two_policies(iter_idx)\n","\n","        for run_idx in range(2, self.warp_args.num_policies):\n","            policy, _ = self._load_policy(self._policy_path(iter_idx, run_idx))\n","            params_iter = zip(init_model.named_parameters(), slerp_model.parameters(), policy.parameters())\n","\n","            for (name, init_param), slerp_param, policy_param in params_iter:\n","                if not utils.is_lora_layer(name):\n","                    continue\n","\n","                slerp_param = slerp_param.detach()\n","                task_vector_1 = slerp_param - init_param\n","                task_vector_2 = policy_param - init_param\n","\n","                angle = utils.compute_angle(task_vector_1, task_vector_2)\n","                mult = torch.sin(angle / self.warp_args.num_policies) / torch.sin(angle)\n","\n","                slerp_param *= mult\n","                slerp_param += (1 - mult) * init_param + mult * task_vector_2\n","\n","        return slerp_model\n","\n","    def _slerp_first_two_policies(self, iter_idx: int) -> transformers.PreTrainedModel:\n","        slerp_model, _ = self._load_policy(self.init_policy_checkpoint)\n","        policy_1, _ = self._load_policy(self._policy_path(iter_idx, 0))\n","        policy_2, _ = self._load_policy(self._policy_path(iter_idx, 1))\n","\n","        params_iter = zip(slerp_model.named_parameters(), policy_1.parameters(), policy_2.parameters())\n","\n","        for (name, slerp_param), policy1_param, policy2_param in params_iter:\n","            if not utils.is_lora_layer(name):\n","                continue\n","\n","            slerp_param = slerp_param.detach()\n","            task_vector_1 = policy1_param - slerp_param\n","            task_vector_2 = policy2_param - slerp_param\n","\n","            angle = utils.compute_angle(task_vector_1, task_vector_2)\n","            slerp_param += torch.sin((1 - self.warp_args.slerp_rate) * angle) / torch.sin(angle) * task_vector_1\n","            slerp_param += torch.sin(self.warp_args.slerp_rate * angle) / torch.sin(angle) * task_vector_2\n","    \n","        return slerp_model\n","\n","    def _evaluate_slerp_model(self, iter_idx: int):\n","        device_idx = self._pop_device()\n","        device = torch.device('cuda')\n","        group_name = self.checkpoints_args.group_name\n","        eval_args = self._get_eval_args(iter_idx)\n","\n","        slerp_model, slerp_tokenizer = self._load_policy(self._slerp_path(iter_idx), is_trainable=False, device_map=device)\n","        slerp_model.eval()\n","        sft_model, _ = self._load_policy(self.checkpoints_args.sft_checkpoint, is_trainable=False, device_map=device)\n","        sft_model.eval()\n","        reward_model, reward_tokenizer = self._load_reward(device_map=device)\n","\n","        with wandb.init(group=group_name, job_type=eval_args.run_name, name=eval_args.run_name, id=self._eval_run_id, resume='allow', project='tk-alignment') as run:\n","            self._eval_run_id = self._eval_run_id or run.id\n","\n","            trainer = policy_trainer.PolicyTrainer(\n","                slerp_model,\n","                slerp_tokenizer,\n","                reward_model,\n","                reward_tokenizer,\n","                self.generation_config,\n","                self.warp_args,\n","                sft_model,\n","                eval_args,\n","                eval_dataset=self.eval_dataset,\n","            )\n","\n","            trainer.remove_callback(transformers.trainer_callback.PrinterCallback)\n","            trainer.remove_callback(transformers.trainer_callback.ProgressCallback)\n","            trainer.evaluate()\n","\n","            self._device_queue.put(device_idx)\n","\n","    def _pop_device(self) -> int:\n","        device_idx = self._device_queue.get()\n","        os.environ['CUDA_VISIBLE_DEVICES'] = str(device_idx)\n","        return device_idx\n","\n","    def _get_train_args(self, iter_idx: int, run_idx: int) -> transformers.TrainingArguments:\n","        output_dir = self._policy_path(iter_idx, run_idx)\n","        run_name = f'iter_{iter_idx}_run_{run_idx}'\n","        seed = iter_idx * self.warp_args.num_policies + run_idx\n","\n","        return transformers.TrainingArguments(\n","            output_dir=output_dir,\n","            run_name=run_name,\n","            seed=seed,\n","            gradient_checkpointing_kwargs={\"use_reentrant\": False} if self.train_args.gradient_checkpointing else None,\n","            num_train_epochs=0,\n","            **asdict(self.train_args),\n","        )\n","\n","    def _get_eval_args(self, iter_idx: int) -> transformers.TrainingArguments:\n","        output_dir = self._slerp_path(iter_idx)\n","        return transformers.TrainingArguments(\n","            output_dir=output_dir,\n","            run_name='eval_slerp',\n","            batch_eval_metrics=True,\n","            include_inputs_for_metrics=True,\n","            num_train_epochs=0,\n","            seed=iter_idx,\n","            **asdict(self.train_args),\n","        )\n","\n","    def _load_policy(self, path: str, is_trainable: bool = True, device_map: torch.device | None = None) -> peft.peft_model.PeftModelForCausalLM:\n","        path = utils.get_latest_checkpoint(path)\n","        policy = peft.AutoPeftModelForCausalLM.from_pretrained(path, is_trainable=is_trainable, device_map=device_map)\n","        for module in policy.modules():\n","            if isinstance(module, torch.nn.Dropout):\n","                module.p = 0\n","\n","        policy.config.use_cache = not self.train_args.gradient_checkpointing\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(path, padding_side='left')\n","        return policy, tokenizer\n","\n","    def _load_reward(self, device_map: torch.device | None = None) -> transformers.PreTrainedModel:\n","        path = utils.get_latest_checkpoint(self.checkpoints_args.reward_checkpoint)\n","        reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(path, device_map=device_map)\n","        reward_model.eval()\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(path, padding_side='left')\n","        return reward_model, tokenizer\n","\n","    def _policy_path(self, iter_idx: int, run_idx: int):\n","        return os.path.join(self.checkpoints_args.save_folder, f'iter_{iter_idx}_run_{run_idx}')\n","\n","    def _slerp_path(self, iter_idx: int):\n","        return os.path.join(self.checkpoints_args.save_folder, f'iter_{iter_idx}_slerp')\n","\n","    def _init_path(self, iter_idx: int):\n","        return os.path.join(self.checkpoints_args.save_folder, f'iter_{iter_idx}_init')\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load sft/reward checkpoints"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:05:51.351724Z","iopub.status.busy":"2024-08-05T09:05:51.350866Z","iopub.status.idle":"2024-08-05T09:06:16.270585Z","shell.execute_reply":"2024-08-05T09:06:16.269687Z","shell.execute_reply.started":"2024-08-05T09:05:51.351691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1TgG3MllcM-N0BZja8JaeeVQBbVrrMyFE\n","From (redirected): https://drive.google.com/uc?id=1TgG3MllcM-N0BZja8JaeeVQBbVrrMyFE&confirm=t&uuid=70d4e5aa-924e-414a-92f4-dcccfc69ee79\n","To: /kaggle/working/checkpoints.zip\n","100%|████████████████████████████████████████| 723M/723M [00:10<00:00, 66.8MB/s]\n","Archive:  checkpoints.zip\n","   creating: checkpoints/\n","   creating: checkpoints/reward/\n","  inflating: checkpoints/reward/special_tokens_map.json  \n","  inflating: checkpoints/reward/training_args.bin  \n","  inflating: checkpoints/reward/tokenizer.json  \n","  inflating: checkpoints/reward/rng_state.pth  \n","  inflating: checkpoints/reward/scheduler.pt  \n","  inflating: checkpoints/reward/optimizer.pt  \n","  inflating: checkpoints/reward/config.json  \n","  inflating: checkpoints/reward/trainer_state.json  \n","  inflating: checkpoints/reward/vocab.txt  \n","  inflating: checkpoints/reward/tokenizer_config.json  \n","  inflating: checkpoints/reward/model.safetensors  \n","   creating: checkpoints/sft/\n","  inflating: checkpoints/sft/special_tokens_map.json  \n","  inflating: checkpoints/sft/training_args.bin  \n","  inflating: checkpoints/sft/adapter_config.json  \n","  inflating: checkpoints/sft/README.md  \n","  inflating: checkpoints/sft/tokenizer.json  \n","  inflating: checkpoints/sft/adapter_model.safetensors  \n","  inflating: checkpoints/sft/rng_state.pth  \n","  inflating: checkpoints/sft/scheduler.pt  \n","  inflating: checkpoints/sft/optimizer.pt  \n","  inflating: checkpoints/sft/vocab.json  \n","  inflating: checkpoints/sft/merges.txt  \n","  inflating: checkpoints/sft/trainer_state.json  \n","  inflating: checkpoints/sft/tokenizer_config.json  \n"]}],"source":["!gdown 1TgG3MllcM-N0BZja8JaeeVQBbVrrMyFE\n","!unzip checkpoints.zip"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:14:26.238193Z","iopub.status.busy":"2024-08-05T09:14:26.237136Z","iopub.status.idle":"2024-08-05T09:14:26.268897Z","shell.execute_reply":"2024-08-05T09:14:26.267996Z","shell.execute_reply.started":"2024-08-05T09:14:26.238156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting main.py\n"]}],"source":["%%writefile main.py\n","\n","import os\n","import transformers\n","import torch.multiprocessing as tmp\n","import configs, dataset, warp_trainer, utils\n","from torch.utils.data import Subset\n","\n","\n","if __name__ == '__main__':\n","#     os.environ['WANDB_MODE'] = 'offline'\n","    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n","    os.environ['WANDB_SILENT'] = 'true'\n","    tmp.set_start_method('spawn')\n","\n","    warp_args = configs.WARPArgs()\n","    dataset_args = configs.DatasetArgs()\n","    generation_args = configs.GenerationArgs(max_new_tokens=128, num_return_sequences=4)\n","    train_args = configs.PolicyTrainArgs(warmup_steps=20, learning_rate=1e-4, per_device_train_batch_size=16, per_device_eval_batch_size=4)\n","    lora_args = configs.LoraArgs()\n","    checkpoints_args = configs.CheckpointsArgs(sft_checkpoint='checkpoints/sft', reward_checkpoint='checkpoints/reward', group_name='warp_test_rloo')\n","\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\n","        utils.get_latest_checkpoint(checkpoints_args.sft_checkpoint)\n","    )\n","    imdb = dataset.build_imdb_dataset(\n","        tokenizer,\n","        min_text_length=dataset_args.min_text_length,\n","        min_tokens=dataset_args.min_tokens,\n","        max_tokens=dataset_args.max_tokens,\n","    )\n","\n","    train_dataset = imdb['train']\n","    test_dataset = Subset(imdb['test'], list(range(dataset_args.eval_size)))\n","\n","    warp_trainer = warp_trainer.WARPTrainer(\n","        warp_args,\n","        generation_args,\n","        train_args,\n","        lora_args,\n","        checkpoints_args,\n","        train_dataset,\n","        test_dataset\n","    )\n","\n","    warp_trainer.train()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:08:43.513613Z","iopub.status.busy":"2024-08-05T09:08:43.513252Z","iopub.status.idle":"2024-08-05T09:10:27.931273Z","shell.execute_reply":"2024-08-05T09:10:27.930584Z","shell.execute_reply.started":"2024-08-05T09:08:43.513583Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","\n","wandb.login()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T09:14:30.300861Z","iopub.status.busy":"2024-08-05T09:14:30.300026Z","iopub.status.idle":"2024-08-05T12:55:51.636080Z","shell.execute_reply":"2024-08-05T12:55:51.634839Z","shell.execute_reply.started":"2024-08-05T09:14:30.300824Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-08-05 09:14:35.309843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 09:14:35.309897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 09:14:35.311324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Map:   0%|                           | 80/24872 [00:00<00:32, 772.05 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\n","Map: 100%|████████████████████████| 24872/24872 [00:30<00:00, 827.67 examples/s]\n","2024-08-05 09:15:17.203908: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 09:15:17.203960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 09:15:17.205371: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-08-05 09:15:24.602788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 09:15:24.602838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 09:15:24.604205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-08-05 09:15:24.652986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 09:15:24.653028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 09:15:24.654346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","config.json: 100%|█████████████████████████████| 577/577 [00:00<00:00, 3.60MB/s]\n","pytorch_model.bin: 100%|██████████████████████| 548M/548M [00:01<00:00, 310MB/s]\n","Currently logged in as: sisha\n","Tracking run with wandb version 0.17.4\n","Run data is saved locally in /kaggle/working/wandb\n","View project at https://wandb.ai/sisha/tk-alignment\n","View runs at https://wandb.ai/sisha/tk-alignment/groups/warp_test_rloo\n","Iteration 1/2: 100%|████████████████████████| 200/200 [1:38:10<00:00, 29.45s/it]\n","Iteration 2/2:   0%|                                    | 0/200 [00:00<?, ?it/s]2024-08-05 11:06:47.275341: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 11:06:47.275413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 11:06:47.276191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-05 11:06:47.276237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-05 11:06:47.277317: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-08-05 11:06:47.277569: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Iteration 2/2: 100%|████████████████████████| 200/200 [1:38:13<00:00, 29.47s/it]\n"]}],"source":["!python -W ignore::UserWarning: main.py"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T13:00:26.352050Z","iopub.status.busy":"2024-08-05T13:00:26.351100Z","iopub.status.idle":"2024-08-05T13:00:27.365785Z","shell.execute_reply":"2024-08-05T13:00:27.364904Z","shell.execute_reply.started":"2024-08-05T13:00:26.352016Z"},"trusted":true},"outputs":[],"source":["!zip -r rloo_1.zip warp/iter_1_slerp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T13:00:18.550698Z","iopub.status.busy":"2024-08-05T13:00:18.549845Z","iopub.status.idle":"2024-08-05T13:00:18.586494Z","shell.execute_reply":"2024-08-05T13:00:18.585622Z","shell.execute_reply.started":"2024-08-05T13:00:18.550664Z"},"trusted":true},"outputs":[],"source":["from IPython.display import FileLink\n","\n","FileLink('rloo_1.zip')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
